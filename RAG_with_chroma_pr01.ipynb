{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "53c6bb41-bc92-4fa8-bb66-f0b25f10ce76",
      "cell_type": "markdown",
      "source": "## Problem Statement: InsureLLM Internal Knowledge Chatbot\n**Project Title**: InsureLLM-Bot: An Intelligent Assistant for Internal Document Retrieval\n\n**Introduction**:\nInsureLLM is a growing insurance technology company with an expanding internal knowledge base. Critical information about the company's history, employee roles, product specifications, and client contracts is currently stored in a collection of Markdown (.md) files distributed across multiple folders. As the company scales, employees find it increasingly difficult and time-consuming to manually search through these documents to find specific answers, leading to decreased productivity and inconsistent information sharing.\n\n**The Problem**:\nThere is no centralized, intelligent system for employees to quickly and accurately query the company's internal knowledge base. An employee needing to know the CEO's prior experience or the specifics of a product's features has to manually locate and read through potentially dozens of files. This process is inefficient and prone to human error.\n\n**Objective**:\nThe goal of this project is to develop a Retrieval-Augmented Generation (RAG) application that serves as an internal chatbot for InsureLLM. This chatbot will allow employees to ask questions in natural language and receive concise, accurate answers sourced directly from the company's private documents.\n\n**Technical Requirements**:\n\n- **Data Ingestion**: The system must automatically load all .md files from a knowledge-base directory containing the subfolders: company, employee, product, and contract.\n\n- **Vector Database**: The loaded documents must be chunked, converted into vector embeddings, and stored in a local ChromaDB vector store for efficient similarity searches.\n\n- **LLM Integration**: The application will use one of Google's powerful Gemini models (e.g., gemini-1.5-flash-latest) via the GoogleGenerativeAI LangChain integration to understand queries and generate answers.\n\n- **RAG Pipeline**: A conversational retrieval chain must be implemented to:\n\n        - Take a user's question.\n        \n        - Retrieve the most relevant document chunks from ChromaDB.\n        \n        - Augment the LLM's context with this retrieved information.\n        \n        - Generate a factually grounded answer.\n\n- **User Interface**: A simple, web-based chat interface will be created using Gradio to allow for interactive and conversational queries.\n\n- **Memory**: The chatbot must have conversational memory to understand follow-up questions within the context of an ongoing conversation.\n\n## Success Criteria:\nThe project will be considered a success when an employee, such as a new hire, can ask the chatbot a series of questions and receive accurate answers. For example:\n\n- \"Can you describe InsureLLM in a few sentences?\"\n\n- \"Who is the CEO?\"\n\n- \"What was Avery Lancaster's experience before joining the company?\"\n\nThe final deliverable will be a well-documented Jupyter Notebook that demonstrates the entire end-to-end process, from data loading to the interactive chat application.",
      "metadata": {}
    },
    {
      "id": "19e15ce5-8876-4ef1-82dc-047ad1792387",
      "cell_type": "markdown",
      "source": "## Sample Knowledge Base Structure & Content\nTo make the demo work, your knowledge-base folder should be structured like this, with .md files inside:\n\n''' knowledge-base/\n├── company/\n│   └── about_us.md\n├── employee/\n│   └── avery_lancaster_ceo.md\n├── product/\n│   └── policyguard_pro.md\n└── contract/\n    └── standard_terms.md \n    '''",
      "metadata": {}
    },
    {
      "id": "52b9d5ff-a330-450d-b179-fdb95b827188",
      "cell_type": "code",
      "source": "# imports\nimport os\nimport glob\nimport gradio as gr\nfrom langchain.document_loaders import DirectoryLoader, TextLoader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings, GoogleGenerativeAI\nfrom langchain.vectorstores import Chroma\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chains import ConversationalRetrievalChain",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7e99c2fa-ee3a-4525-bd59-3e3ae65d43ec",
      "cell_type": "code",
      "source": "# Load API Key\n# --------------------\n# IMPORTANT: This cell sets your Google API key as an environment variable.\n# LangChain's Google libraries automatically look for this variable to authenticate.\n# Replace \"YOUR_API_KEY_HERE\" with your actual Gemini API key.\n\nos.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY_HERE\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "36acd59a-777a-4f30-8a10-1950e05befa0",
      "cell_type": "code",
      "source": "# Load Documents from Knowledge Base\n# ------------------------------------------\n# This cell scans the 'knowledge-base' directory and all its subfolders for Markdown (.md) files.\n# It uses LangChain's DirectoryLoader to read the content of each file.\n# Each document is also tagged with metadata indicating its source folder (e.g., 'company', 'employee').\n\nprint(\"Loading documents from the knowledge base...\")\nfolders = glob.glob(\"knowledge-base/*\")\ntext_loader_kwargs = {'encoding': 'utf-8'}\ndocuments = []\n\nfor folder in folders:\n    doc_type = os.path.basename(folder)\n    loader = DirectoryLoader(\n        folder,\n        glob=\"**/*.md\",\n        loader_cls=TextLoader,\n        loader_kwargs=text_loader_kwargs\n    )\n    folder_docs = loader.load()\n    for doc in folder_docs:\n        doc.metadata[\"doc_type\"] = doc_type\n        documents.append(doc)\nprint(f\"Successfully loaded {len(documents)} documents.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0103e317-c498-4564-a45e-d5c31ed3225f",
      "cell_type": "code",
      "source": "# Cell 4: Split Documents into Chunks\n# -----------------------------------\n# Large documents are too big to fit into the context window of an LLM.\n# Here, we split the loaded documents into smaller chunks of text.\n# `chunk_overlap` ensures that there is some continuity between chunks to not lose context.\n\nprint(\"Splitting documents into chunks...\")\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nchunks = text_splitter.split_documents(documents)\nprint(f\"Created {len(chunks)} document chunks.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5165f962-8432-4189-a547-38a66514b56e",
      "cell_type": "code",
      "source": "#: Create and Persist the Vector Database\n# -----------------------------------------------\n# This is the core of the \"Retrieval\" part of RAG.\n# 1. Initialize the Gemini embedding model ('models/embedding-001').\n# 2. Use Chroma.from_documents to:\n#    a. Convert all text chunks into numerical vectors (embeddings) using the Gemini model.\n#    b. Store these vectors in a local directory named 'vector_db'.\n# This process only needs to be run once. For subsequent runs, we can load the saved database.\n\nprint(\"Creating and persisting the vector database...\")\nembeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\nvector_store = Chroma.from_documents(\n    documents=chunks,\n    embedding=embeddings,\n    persist_directory=\"./vector_db\"\n)\nprint(\"✅ Vector store created successfully in the 'vector_db' folder!\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "72899a1e-9a71-4536-9b54-9eeaf10b045e",
      "cell_type": "code",
      "source": "#: Set Up the Conversational RAG Chain\n# -------------------------------------------\n# This cell assembles all the components into a complete, stateful chatbot pipeline.\n# 1. Load the persisted ChromaDB from disk.\n# 2. Create a 'retriever' which is a component that can search the vector store for relevant chunks.\n# 3. Initialize the Gemini LLM ('gemini-1.5-flash-latest') for generating answers.\n# 4. Set up ConversationBufferMemory to store the chat history.\n# 5. Create the ConversationalRetrievalChain, which orchestrates the entire process:\n#    - Takes a question.\n#    - Uses memory to rephrase it if needed.\n#    - Sends it to the retriever.\n#    - Bundles the question and retrieved documents into a prompt for the LLM.\n#    - Gets the final answer from the LLM.\n\nprint(\"Setting up the conversational RAG chain...\")\ndb = Chroma(persist_directory=\"./vector_db\", embedding_function=embeddings)\nretriever = db.as_retriever(search_kwargs={'k': 3}) # Retrieve top 3 relevant chunks\nllm = GoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0.1)\nmemory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n\nqa_chain = ConversationalRetrievalChain.from_llm(\n    llm=llm,\n    retriever=retriever,\n    memory=memory\n)\nprint(\"✅ Chatbot is ready.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f8d0ca14-60fd-4319-90d7-ed59ff313c27",
      "cell_type": "code",
      "source": "# Launch the Gradio Web Interface\n# ---------------------------------------\n# This final cell creates and launches the user interface for our chatbot.\n# 1. Define a 'chat' function that takes a user's message and history.\n# 2. Inside the function, it calls our 'qa_chain' to get the answer.\n# 3. It then launches a Gradio ChatInterface, which provides a clean, web-based UI\n#    that appears directly in the Jupyter Notebook output.\n\ndef chat(message, history):\n    \"\"\"Function to handle the chat logic for the Gradio interface.\"\"\"\n    result = qa_chain.invoke({\"question\": message})\n    return result[\"answer\"]\n\nprint(\"Launching Gradio Chat Interface...\")\nview = gr.ChatInterface(\n    fn=chat,\n    title=\"InsureLLM RAG Chatbot 🤖\",\n    description=\"Ask me anything about InsureLLM's company info, employees, products, or contracts.\"\n).launch()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}